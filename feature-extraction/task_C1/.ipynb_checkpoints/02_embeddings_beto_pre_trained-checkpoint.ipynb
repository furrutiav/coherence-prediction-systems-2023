{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf1dcd1",
   "metadata": {},
   "source": [
    "## Libreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d23897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2584f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1f21494",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train = pd.read_excel(\"../../data/train_task_C1.xlsx\", index_col=\"id\")\n",
    "A_val = pd.read_excel(\"../../data/val_task_C1.xlsx\", index_col=\"id\")\n",
    "A_test = pd.read_excel(\"../../data/test_task_C1.xlsx\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ab56d",
   "metadata": {},
   "source": [
    "## Embeddings BETO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92cd71",
   "metadata": {},
   "source": [
    "Load model [BETO](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased) from [huggingface](https://huggingface.co/) ðŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54fdac46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "beto_model = BertModel.from_pretrained(model_name)\n",
    "beto_tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "_ = beto_model.eval()\n",
    "__ = beto_model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6bbb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'], [3, 5, 1, 4, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beto_tokenizer.all_special_tokens, beto_tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2948efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_from_pair(text1, text2, maxlen=100):\n",
    "    text1 = \" \".join(str(text1).replace(\"\\n\", \" \").split())\n",
    "    text2 = \" \".join(str(text2).replace(\"\\n\", \" \").split())\n",
    "    \n",
    "    tokens1 = [\"[CLS]\", \"[UNK]\"]\n",
    "    if text1 != \"\":\n",
    "        tokens1 = beto_tokenizer.tokenize(text1)\n",
    "        tokens1 = [\"[CLS]\"] + tokens1[:maxlen]\n",
    "    if text2 != \"\":\n",
    "        tokens2 = beto_tokenizer.tokenize(text2)\n",
    "        tokens2 = tokens2[:maxlen]\n",
    "        \n",
    "    tokens = tokens1 + [\"[SEP]\"] + tokens2 + [\"[SEP]\"]\n",
    "    input_ids = beto_tokenizer.convert_tokens_to_ids(tokens)    \n",
    "    input_ids_tensor = torch.tensor(input_ids)\n",
    "    \n",
    "    cont = beto_model(input_ids_tensor.unsqueeze(0))\n",
    "    cls_cont = cont.last_hidden_state[:, 0]\n",
    "    return cls_cont[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33524c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embd(df, text1_col=\"Q\", text2_col=\"A\"):\n",
    "    embeddings = []\n",
    "    for ix in df.index:\n",
    "        text1 = df.loc[ix][text1_col]\n",
    "        text2 = df.loc[ix][text2_col]\n",
    "        embd = get_cls_from_pair(text1, text2)\n",
    "        embeddings.append(embd)\n",
    "    o = pd.DataFrame(embeddings)\n",
    "    o[\"id\"] = df.index\n",
    "    o = o.set_index(\"id\")\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "000def99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3h 12min 27s\n",
      "Wall time: 32min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embd_A_train = get_embd(A_train, text1_col=\"Q\", text2_col=\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0959f6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 43min 13s\n",
      "Wall time: 7min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embd_A_val = get_embd(A_val, text1_col=\"Q\", text2_col=\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59cc28b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7min 20s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embd_A_test = get_embd(A_test, text1_col=\"Q\", text2_col=\"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1653c398",
   "metadata": {},
   "source": [
    "## Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "611c83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_A_train.to_excel(\"../features/beto_pt_features_train_task_C1.xlsx\")\n",
    "embd_A_val.to_excel(\"../features/beto_pt_features_val_task_C1.xlsx\")\n",
    "embd_A_test.to_excel(\"../features/beto_pt_features_test_task_C1.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
