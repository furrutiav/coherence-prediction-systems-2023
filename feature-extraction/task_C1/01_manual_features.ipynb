{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77fb92c",
   "metadata": {},
   "source": [
    "## Libreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7f823625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ce51f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U spacy\n",
    "import spacy\n",
    "!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ecaa68e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\felip\\anaconda3\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "80a960c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\felip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c1d783de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Levenshtein in c:\\users\\felip\\anaconda3\\lib\\site-packages (0.18.1)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.0.1 in c:\\users\\felip\\anaconda3\\lib\\site-packages (from Levenshtein) (2.0.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install Levenshtein\n",
    "import Levenshtein as lev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf9456",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "89409385",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train = pd.read_excel(\"../../data/train_task_C1.xlsx\", index_col=\"id\")\n",
    "A_val = pd.read_excel(\"../../data/val_task_C1.xlsx\", index_col=\"id\")\n",
    "A_test = pd.read_excel(\"../../data/test_task_C1.xlsx\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c26bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "afd030d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_md')\n",
    "spanish_stopwords = stopwords.words(\"spanish\")\n",
    "class_numbercheckspeller = imp.load_source('module.name', '../lib/class_numbercheckspeller.py')\n",
    "base_symspell = pickle.load(open(\"../lib/base_symspell.pickle\", \"rb\"))\n",
    "D_correction = pickle.load(open(\"../lib/D_correction.pickle\", \"rb\"))\n",
    "ws = class_numbercheckspeller.SymSpellNumbers(base_symspell, D_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ef435e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_rae_ud = pickle.load(open(\"../lib/resource_rae_ud.pickle\", \"rb\"))\n",
    "special_words = pickle.load(open(\"../lib/special_words.pickle\", \"rb\"))\n",
    "keywords_C1 = special_words[\"keywords_C1\"]\n",
    "slang_C1 = special_words[\"slang_C1\"]\n",
    "bad_words = special_words[\"bad_words\"]\n",
    "recurrent_words = special_words[\"recurrent_words\"]\n",
    "generated_faces = pickle.load(open(\"../lib/faces.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7f92d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump({\n",
    "#     \"keywords_C1\": keywords_C1, \n",
    "#     \"slang_C1\": slang_C1, \n",
    "#     \"bad_words\": bad_words, \n",
    "#     \"recurrent_words\": recurrent_words\n",
    "# }, open(\"../lib/special_words.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "dbd70180",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = punctuation+\"´\"+\"¡\"\n",
    "vowel = \"aeiou\"\n",
    "digit = \"0123456789\"\n",
    "blank = \" \"\n",
    "math_punct = \"\"\"$%()*+,-./:<=>[\\]{}x\"\"\"\n",
    "list_rae = dic_rae_ud[\"rae\"]\n",
    "list_ud = dic_rae_ud[\"ud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a25e6010",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_faces = []\n",
    "for f in generated_faces:\n",
    "    if all(c in punct for c in f):\n",
    "        punct_faces.append(f)\n",
    "        \n",
    "no_digit_faces = []\n",
    "for f in generated_faces:\n",
    "    if not f.isdigit():\n",
    "        no_digit_faces.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "88261775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_multiple(string, list_replace, replace_ch):\n",
    "    for ch in list_replace:\n",
    "        if ch in string:\n",
    "            string = string.replace(ch, replace_ch)\n",
    "    return string\n",
    "\n",
    "def get_relevants_subjects(q):\n",
    "    oo = {\"PROPN\": [], \"NOUN\": []}\n",
    "    bi_oo = {\"NOUN-ADP\": [], \"NOUN-PROPN\": []}\n",
    "    nlp_q = []\n",
    "    for t in nlp(q):\n",
    "        r = t.text, t.tag_, t.dep_, t.is_alpha, t.is_stop\n",
    "        nlp_q.append(r)\n",
    "    for r in nlp_q:\n",
    "        if r[1] == \"PROPN\":\n",
    "            oo[\"PROPN\"].append(r)\n",
    "        elif r[1] == \"NOUN\":\n",
    "            oo[\"NOUN\"].append(r)\n",
    "    for k in range(len(nlp_q)-1):\n",
    "        r1, r2 = nlp_q[k], nlp_q[k+1]\n",
    "        if r1[1] == \"NOUN\" and r2[1] == \"ADP\":\n",
    "            bi_oo[\"NOUN-ADP\"].append((r1, r2))\n",
    "        elif r1[1] == \"NOUN\" and r2[1] == \"PROPN\":\n",
    "            bi_oo[\"NOUN-PROPN\"].append((r1, r2))\n",
    "    relevants = []\n",
    "    for x in oo[\"PROPN\"]:\n",
    "        if x[3]:\n",
    "            relevants.append(x)\n",
    "    for x in oo[\"NOUN\"]:\n",
    "        if x[2] in [\"nsubj\"]:\n",
    "            relevants.append(x)\n",
    "    for x, y in bi_oo[\"NOUN-PROPN\"]:\n",
    "        relevants.append(x)\n",
    "        relevants.append(y)\n",
    "    for x, y in bi_oo[\"NOUN-ADP\"]:\n",
    "        if x[2] not in [\"nmod\", \"obj\", \"obl\"]:\n",
    "            relevants.append(x)\n",
    "        if y[2] not in [\"case\"] and not y[4]:\n",
    "            relevants.append(y)\n",
    "    return list(set([x[0].lower() for x in relevants]))\n",
    "\n",
    "def get_attrib(a):\n",
    "    a = \"\" if a == \"nan\" else a\n",
    "    rel_subj = get_relevants_subjects(a)\n",
    "    a_org_tokens = a.split()\n",
    "    a_lower_org_tokens = a.lower().split()\n",
    "    a_lower_org_tokens_wo_punct = replace_multiple(\" \".join(a_lower_org_tokens), punct, \" \").split()\n",
    "    a_ud = [x for x in a_org_tokens if (x.lower() in list_ud) and not (x.lower() in list_rae)]\n",
    "    a = unidecode(a)\n",
    "    a = a.replace(\"\\n\", \" \")\n",
    "    a = \" \".join(a.lower().split())\n",
    "    a = ws.apply(a)\n",
    "    a = \" \".join([x.strip() for x in re.split(r'(-?\\d*\\.?\\d+)', a)])\n",
    "    lf_a = [(t.text, t.lemma_, t.tag_) for t in nlp(\" \".join(a_org_tokens))]\n",
    "    for propn_ in [\"matemáticas\", \"matemáticas\", \"matemática\", \"matematica\", \"matematicas\",\n",
    "                   \"sofia\", \"renata\", \"camila\", \"pia\",\"pía\", \"carla\", \"pamela\", \"patricia\", \n",
    "                   \"matilde\", \"nama\", \"mamá\", \"amigo\", \"amiga\", \"amigos\", \"amigas\"]:\n",
    "        if propn_ in a_lower_org_tokens+a_org_tokens+a.split():\n",
    "            lf_a.append((propn_, propn_, \"PROPN\"))\n",
    "    aux_list = []\n",
    "    if any([x in \" \".join(a_lower_org_tokens) for x in [\"lo mismo\", \"la misma\", \n",
    "                                                        \"los dos\", \"las dos\", \n",
    "                                                        \"los 2\", \"las 2\"]]) or any([x in a for x in  [\"lo mismo\", \"la misma\", \"los dos\", \"las dos\", \"los 2\", \"las 2\"]]):\n",
    "        aux_list.append(\"ambos\")\n",
    "        \n",
    "    return {\n",
    "        \"clean\": a,\n",
    "        \"org_tokens\": a_org_tokens,\n",
    "        \"lower_org_tokens\": a_lower_org_tokens,\n",
    "        \"org_tokens_wo_punct\": a_lower_org_tokens_wo_punct, \n",
    "        \"tokens\": a.split(),\n",
    "        \"blank\": list(map(a.lower().count, blank))[0],\n",
    "        \"vowel\": list(map(a.lower().count, vowel)),\n",
    "        \"punct\": list(map(a.lower().count, punct)),\n",
    "        \"math_punct\": list(map(a.lower().count, math_punct)),\n",
    "        \"digit\": list(map(a.lower().count, digit)),\n",
    "        \"numbers\": re.findall(r\"\\d+\", a),\n",
    "        \"no_numbers\": [t for t in a.split() if str(t).isalpha()],\n",
    "        \"ud\": a_ud, \n",
    "        \"rae\": [x for x in a.split() if x in list_rae],\n",
    "        \"faces\": [f for f in generated_faces if f in \"\".join(a_org_tokens)],\n",
    "        \"slang\": [f for f in slang_C1 if f in \"\".join(a_org_tokens)],\n",
    "        \"keywords\": [f for f in keywords_C1 if f in \"\".join(a_org_tokens)],\n",
    "        \"lf_propn\": list({lf_w[0].lower() for lf_w in lf_a if lf_w[-1] == \"PROPN\"}),\n",
    "        \"lf_lemma\": list({lf_w[1].lower() for lf_w in lf_a}),\n",
    "        \"rel_subj\": list(set(rel_subj+[lf_w[0] for lf_w in lf_a if lf_w[-1] == \"PROPN\"])),\n",
    "        \"aux_tokens\": aux_list\n",
    "    }\n",
    "\n",
    "def get_simple_topo(dic_a):\n",
    "    a = dic_a[\"clean\"]\n",
    "    tokens = dic_a[\"tokens\"]\n",
    "    blank = dic_a[\"tokens\"]\n",
    "    numbers = dic_a[\"numbers\"]\n",
    "    digit = dic_a[\"digit\"]\n",
    "    o =  {\n",
    "        \"len\": len(a),\n",
    "        \"num_tokens\": len(tokens),\n",
    "        \"num_numbers\": len(numbers),\n",
    "        \"num_math_punct\": sum(dic_a[\"math_punct\"]),\n",
    "        \"num_digit\": sum(digit),\n",
    "        \"num_rae\": len(dic_a[\"rae\"]),\n",
    "        \"num_ud\": len(dic_a[\"ud\"]),\n",
    "        \"num_punct\": sum(dic_a[\"punct\"]),\n",
    "        \"num_slang\": len(dic_a[\"slang\"]),\n",
    "        \"num_faces\": len(dic_a[\"faces\"]),\n",
    "        \"num_keywords\": len(dic_a[\"keywords\"]),\n",
    "        \"num_no_numbers\": len(dic_a[\"no_numbers\"])\n",
    "    }\n",
    "    return o\n",
    "\n",
    "def get_ratio_vowel(dic_a):\n",
    "    a = dic_a[\"clean\"]\n",
    "    count_vowel = dic_a[\"vowel\"]\n",
    "    count_blank = dic_a[\"blank\"]\n",
    "    count_punct = dic_a[\"punct\"]\n",
    "    count_digit = dic_a[\"digit\"]\n",
    "    if sum(count_vowel) > 0:\n",
    "        return sum(count_vowel) / (len(a)-count_blank-sum(count_digit)-sum(count_punct))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_max_len_number(dic_a):\n",
    "    if dic_a[\"numbers\"]:\n",
    "        return max([len(n) for n in dic_a[\"numbers\"]])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_ratio_punct(dic_a, default=True):\n",
    "    a = dic_a[\"clean\"]\n",
    "    count_blank = dic_a[\"blank\"]\n",
    "    count_punct = dic_a[\"punct\"] if default else dic_a[\"math_punct\"]\n",
    "    if sum(count_punct) > 0:\n",
    "        return sum(count_punct) / (len(a)-count_blank)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_ratio_rae_ud(dic_a, l):\n",
    "    if l==\"rae\" and len(dic_a[\"org_tokens\"])>0:\n",
    "        return len(dic_a[l])/len(dic_a[\"org_tokens\"])\n",
    "    elif l==\"ud\" and len(dic_a[\"tokens\"])>0:\n",
    "        return len(dic_a[l])/len(dic_a[\"tokens\"])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_ratio_faces(dic_a):\n",
    "    if len(dic_a[\"org_tokens\"])>0:\n",
    "        return len(dic_a[\"faces\"]) / len(dic_a[\"org_tokens\"])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_ratio_slang(dic_a):\n",
    "    if len(dic_a[\"org_tokens\"])>0:\n",
    "        return len(dic_a[\"slang\"]) / len(dic_a[\"org_tokens\"])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_ratio_keywords(dic_a):\n",
    "    if len(dic_a[\"org_tokens\"])>0:\n",
    "        return len(dic_a[\"keywords\"]) / len(dic_a[\"org_tokens\"])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_ratio_no_numbers(dic_a):\n",
    "    if len(dic_a[\"org_tokens\"])>0:\n",
    "        return len(dic_a[\"no_numbers\"]) / len(dic_a[\"org_tokens\"])\n",
    "    else:\n",
    "        return 0 \n",
    "    \n",
    "def sim_lev(a, b):\n",
    "    return 1 - lev.distance(a, b) / max(len(a), len(b)) if len(a) != 0 else 0\n",
    "\n",
    "def get_injection_index(a, q):\n",
    "    relevant_words = [w for w in replace_multiple(unidecode(q).lower(), punct, \" \").split() if not w in spanish_stopwords]\n",
    "    theta = 0.7\n",
    "    tokens_a = a.split()\n",
    "    injection_index = 0\n",
    "    for k, t in enumerate(tokens_a):\n",
    "        for s in relevant_words:\n",
    "            if sim_lev(s, t) >= theta:\n",
    "                injection_index += 1\n",
    "                break\n",
    "        if len(tokens_a):\n",
    "            injection_index *= 1/len(relevant_words)\n",
    "    return injection_index\n",
    "\n",
    "def get_exist_numbs(dic_a, tresh=5):\n",
    "    num_numbers = len(dic_a[\"numbers\"])\n",
    "    if num_numbers == 0:\n",
    "        a = dic_a[\"clean\"]\n",
    "        if \"poco\" in a or \"mucho\" in a:\n",
    "            return 2\n",
    "        else:\n",
    "            return int(any([(d in a) for d in \"1234567890\"]))\n",
    "    else:\n",
    "        return int(max(len(str(n)) for n in dic_a[\"numbers\"])<tresh)\n",
    "    \n",
    "def get_sim_keywords(dic_a, keywords, theta=0.7):\n",
    "    a = dic_a[\"clean\"]\n",
    "    tokens_a = a.split()\n",
    "    injection_index = 0\n",
    "    for k, t in enumerate(tokens_a):\n",
    "        for s in keywords:\n",
    "            if sim_lev(s, t) >= theta:\n",
    "                injection_index += 1\n",
    "                break\n",
    "    return injection_index\n",
    "\n",
    "def get_sim_implication(dic_a, dic_q, typo=\"3\"):\n",
    "    # esta correcto lo que dijo/dice? not-!quién este en él correcto > (si, no)\n",
    "    # cual de las dos afirmaciones esta correcta? (alguna de las afirmaciones)\n",
    "    # quien esta en lo correcto? (quién estar en él correcto) > any PROPN\n",
    "    theta = 0.7\n",
    "    q_lemma = [w.replace(\",\", \"\").replace(\".\", \"\") for w in dic_q[\"lf_lemma\"]]\n",
    "    injection_index = 0\n",
    "    if typo==\"3\":\n",
    "        if ((\"ser\" in q_lemma or \"es\" in q_lemma or \"estar\" in q_lemma or \"este\" in q_lemma or \"esta\" in q_lemma) and \"correcto\" in q_lemma) or (\"tener\" in q_lemma and \"razón\" in q_lemma):\n",
    "            if (\"quién\" in q_lemma) or (\"cuál\" in q_lemma):\n",
    "                propn_q = dic_q[\"lf_propn\"]+[\"ninguno\",\"ninguna\", \"todos\",\"todas\", \"ambos\",\"ambas\", \"nadie\", \"alguno\", \"alguna\"]\n",
    "                tokens_a = set(dic_a[\"org_tokens\"]+dic_a[\"tokens\"]+dic_a[\"aux_tokens\"]+dic_a[\"lower_org_tokens\"]+dic_a[\"org_tokens_wo_punct\"])\n",
    "                for k, t in enumerate(tokens_a):\n",
    "                    t = t.replace(\",\", \"\").replace(\".\", \"\")\n",
    "                    for s in propn_q:\n",
    "                        if sim_lev(s, t) >= theta:\n",
    "                            injection_index += 1\n",
    "                            break\n",
    "            else:\n",
    "                tokens_a = set(dic_a[\"org_tokens\"]+dic_a[\"tokens\"]+dic_a[\"lower_org_tokens\"]+dic_a[\"org_tokens_wo_punct\"]+dic_a[\"org_tokens_wo_punct\"])\n",
    "                for k, t in enumerate(tokens_a):\n",
    "                    t = t.replace(\",\", \"\").replace(\".\", \"\")\n",
    "                    for s in [\"falso\", \"verdadero\", \"sip\", \"nop\", \"estamal\", \"estabien\", \"bien\", \"mal\", \"si\", \"no\", \"correcta\", \"confundida\", \"confundido\", \"correcto\", \"equivocada\", \"equivocado\", \"incorrecta\", \"incorrecto\", \"razon\", \"razón\"]:\n",
    "                        if sim_lev(s, t) >= theta:\n",
    "                            injection_index += 1\n",
    "                            break\n",
    "        elif (\"por\" in q_lemma and \"qué\" in q_lemma and (\"ser\" in q_lemma or \"es\" in q_lemma or \"estar\" in q_lemma or \"este\" in q_lemma or \"esta\" in q_lemma) and (\"equivocado\" in q_lemma or \"equivocada\" in q_lemma)):\n",
    "            tokens_a = set(dic_a[\"org_tokens\"]+dic_a[\"tokens\"]+dic_a[\"lower_org_tokens\"]+dic_a[\"org_tokens_wo_punct\"])\n",
    "            for k, t in enumerate(tokens_a):\n",
    "                t = t.replace(\",\", \"\").replace(\".\", \"\")\n",
    "                for s in [\"porque\", \"por\", \"que\", \"bien\", \"sip\", \"nop\", \"estamal\", \"estabien\", \"mal\", \"si\", \"no\", \"correcta\", \"confundida\", \"confundido\", \"correcto\", \"equivocada\", \"equivocado\", \"incorrecta\", \"incorrecto\", \"razon\", \"razón\"]:\n",
    "                    if sim_lev(s, t) >= theta:\n",
    "                        injection_index += 1\n",
    "                        break\n",
    "        elif (\"ser\" in q_lemma or \"es\" in q_lemma or \"estar\" in q_lemma or \"este\" in q_lemma or \"esta\" in q_lemma) and (\"bien\" in q_lemma):\n",
    "            tokens_a = set(dic_a[\"org_tokens\"]+dic_a[\"tokens\"]+dic_a[\"lower_org_tokens\"]+dic_a[\"org_tokens_wo_punct\"])\n",
    "            for k, t in enumerate(tokens_a):\n",
    "                t = t.replace(\",\", \"\").replace(\".\", \"\")\n",
    "                for s in [\"verdadero\", \"falso\", \"bien\", \"mal\", \"si\", \"no\", \"correcta\", \"correcto\", \"equivocada\", \"equivocado\", \"confundida\", \"confundido\", \"incorrecta\", \"incorrecto\", \"razon\", \"razón\", \"sip\", \"nop\", \"estamal\", \"estabien\"]:\n",
    "                    if sim_lev(s, t) >= theta:\n",
    "                        injection_index += 1\n",
    "                        break\n",
    "    elif typo==\"4\":\n",
    "        if \"ser\" in q_lemma and \"posible\" in q_lemma:\n",
    "            tokens_a = set(dic_a[\"org_tokens\"]+dic_a[\"tokens\"]+dic_a[\"lower_org_tokens\"]+dic_a[\"org_tokens_wo_punct\"])\n",
    "            for k, t in enumerate(tokens_a):\n",
    "                t = t.replace(\",\", \"\").replace(\".\", \"\")\n",
    "                for s in [\"verdadero\", \"falso\", \"bien\", \"mal\", \"si\", \"no\", \"correcta\", \"correcto\", \"equivocada\", \"equivocado\", \"confundida\", \"confundido\", \"incorrecta\", \"incorrecto\", \"razon\", \"razón\", \"sip\", \"nop\", \"estamal\", \"estabien\"]:\n",
    "                    if sim_lev(s, t) >= theta:\n",
    "                        injection_index += 1\n",
    "                        break\n",
    "            \n",
    "        elif \"quién\" in q_lemma or \"cuál\" in q_lemma or \"qué\" in q_lemma:\n",
    "            propn_q = dic_q[\"rel_subj\"]+[\"ninguno\",\"ninguna\", \"todos\",\"todas\", \"ambos\",\"ambas\", \"nadie\", \"alguno\", \"alguna\"]\n",
    "            tokens_a = set(dic_a[\"org_tokens\"]+dic_a[\"tokens\"]+dic_a[\"aux_tokens\"]+dic_a[\"lower_org_tokens\"]+dic_a[\"org_tokens_wo_punct\"])\n",
    "            for k, t in enumerate(tokens_a):\n",
    "                t = t.replace(\",\", \"\").replace(\".\", \"\")\n",
    "                for s in propn_q:\n",
    "                    if sim_lev(s, t) >= theta:\n",
    "                        injection_index += 1\n",
    "                        break\n",
    "    return injection_index\n",
    "\n",
    "def get_topo_features(a):\n",
    "    def get_crit_prop_vowel(w, prop_vowels):\n",
    "        if len(w.split()) == 1:\n",
    "            l_ = len(w.replace(\" \", \"\"))\n",
    "            if l_ and prop_vowels > 0.675:\n",
    "                return 1\n",
    "            elif w.isalpha():\n",
    "                l_prop = {5: 2/5, 6: 2/6, 7: 2/7, 8: 3/8, 9: 4/9, 10: 4/10, 11: 5/11, 12: 6/12, 13: 5/13, 14: 6/14, 15: 8/15, 16: 8/16, 17: 8/17, 18: 9/18, 19: 9/19, 20: 9/20}\n",
    "                if l_>21 and prop_vowels<1/2: \n",
    "                    return 1\n",
    "                elif 21>l_>4:\n",
    "                    return int(prop_vowels<l_prop[l_])\n",
    "        return 0\n",
    "\n",
    "    def get_prop_vowels(w):\n",
    "        N = len(a.replace(\" \", \"\"))\n",
    "        if N>0:\n",
    "            return sum( int(w in \"aeiou\") for w in a.replace(\" \", \"\")) / N\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def len_max_rep_char(w):\n",
    "        w=w+\" \"\n",
    "        c0 = w[0]\n",
    "        lens = [0]\n",
    "        clen = 1\n",
    "        for c in w[1:]:\n",
    "            if c == c0:\n",
    "                clen += 1\n",
    "            else:\n",
    "                if c0.isalpha():\n",
    "                    if clen>3 and c0 in [\"r\", \"l\", \"c\"]:\n",
    "                        lens.append(clen)\n",
    "                    elif clen>1:\n",
    "                        lens.append(clen)\n",
    "                c0 = c\n",
    "                clen = 1\n",
    "        return max(lens)  \n",
    "    \n",
    "    def max_char_fre_per_token(w, c=\"k\"):\n",
    "        tw = w.split()\n",
    "        fmax = 0\n",
    "        for t in tw:\n",
    "            f = sum(int(ch==c) for ch in t)\n",
    "            if f>fmax:\n",
    "                fmax = f\n",
    "        return fmax\n",
    "    \n",
    "    def max_type_rep_char_per_token(w, t=\"vowel\"):\n",
    "        w=unidecode(w+\" \")\n",
    "        c0 = w[0]\n",
    "        lens = [0]\n",
    "        clen = 1\n",
    "        for c in w[1:]:\n",
    "            if (c0.isalpha() and c.isalpha()) and ((c in \"aeiou\" and c0 in \"aeiou\") or (c not in \"aeiou\" and c0 not in \"aeiou\")):\n",
    "                clen += 1\n",
    "            else:\n",
    "                if t==\"vowel\":\n",
    "                    if c0 in \"aeiou\":\n",
    "                        lens.append(clen)\n",
    "                else:\n",
    "                    if c0 not in \"aeiou\":\n",
    "                        lens.append(clen) \n",
    "                c0 = c\n",
    "                clen = 1\n",
    "        return max(lens) \n",
    "    \n",
    "    a = str(a).replace(\"\\n\", \" \").lower()\n",
    "    a = \" \".join(a.split())\n",
    "    o = {}\n",
    "    \n",
    "    na = a.replace(\" \", \"\")\n",
    "    \n",
    "    o[\"traditional<&>len(~A)\"] = len(na)\n",
    "    o[\"traditional<&>prop_punct\"] = sum(int(w in punct) for w in na)/o[\"traditional<&>len(~A)\"] if o[\"traditional<&>len(~A)\"]>0 else 0\n",
    "    o[\"traditional<&>prop_punct+no-vowel\"] = sum(int(w in punct or (w not in \"aeiou\" and w.isalpha())) for w in na)/o[\"traditional<&>len(~A)\"] if o[\"traditional<&>len(~A)\"]>0 else 0\n",
    "    o[\"traditional<&>prop_vowels\"] = get_prop_vowels(a)\n",
    "    o[\"traditional<&>len(tokens(A))\"] = len(a.split())\n",
    "    o[\"traditional<&>len_max_rep_char\"] = len_max_rep_char(a)\n",
    "    o[\"semantic<&>A.isface()\"] = int(a in generated_faces and not a.isdigit() and a not in [\"ANA\", \"ana\"])\n",
    "    o[\"traditional<&>A.isdigit()\"] = int(na.isdigit())\n",
    "    o[\"traditional<&>frec_char(k)\"] = max_char_fre_per_token(a, c=\"k\")\n",
    "    o[\"traditional<&>frec_char(g)\"] = max_char_fre_per_token(a, c=\"g\")\n",
    "    o[\"traditional<&>frec_char(y)\"] = max_char_fre_per_token(a, c=\"y\")\n",
    "    o[\"traditional<&>frec_char(j)\"] = max_char_fre_per_token(a, c=\"j\")\n",
    "    o[\"traditional<&>frec_char(h)\"] = max_char_fre_per_token(a, c=\"h\")\n",
    "    o[\"traditional<&>frec_char(x)\"] = max_char_fre_per_token(a, c=\"x\")\n",
    "    o[\"traditional<&>frec_char(w)\"] = max_char_fre_per_token(a, c=\"w\")\n",
    "    o[\"traditional<&>frec_char(ñ)\"] = max_char_fre_per_token(a, c=\"ñ\")\n",
    "    o[\"semantic<&>A.is(nose)\"] = int(a == \"nose\")\n",
    "    o[\"traditional<&>A.is(nan)\"] = int(a == \"nan\")\n",
    "    o[\"semantic<&>A.is(ola|hola)\"] = int(a in [\"hola\", \"ola\"])\n",
    "    o[\"semantic<&>A.contains(bad-word)\"] = sum(int(w in bad_words) for w in a.split())\n",
    "    o[\"semantic<&>A.contains(punct_faces)\"] = sum(int(f in a) for f in punct_faces)\n",
    "    o[\"traditional<&>prop_punct+digit\"] = sum(int(w in punct or w.isdigit())for w in na)/o[\"traditional<&>len(~A)\"] if o[\"traditional<&>len(~A)\"]>0 else 0\n",
    "    o[\"traditional<&>prop_no_math_punct\"] = sum(int(w not in math_punct and w in punct )for w in na)/o[\"traditional<&>len(~A)\"] if o[\"traditional<&>len(~A)\"]>0 else 0\n",
    "    o[\"traditional<&>max_no-vowel_rep_char_per_token\"] = max_type_rep_char_per_token(a, \"\")\n",
    "    o[\"semantic<&>prop_no_digit_faces\"] = sum(int(t in no_digit_faces and t not in [\"ANA\", \"ana\"]) for t in a.split())/len(a.split())\n",
    "    o[\"semantic<&>prop_keywords\"] = sum(int(t in recurrent_words) for t in a.split())/len(a.split())\n",
    "    o[\"traditional<&>prop_digit_char\"] = sum(int(t.isdigit()) for t in na)/o[\"traditional<&>len(~A)\"]\n",
    "    o[\"traditional<&>max_vowel_rep_char_per_token\"] = max_type_rep_char_per_token(a, \"vowel\")\n",
    "    o[\"traditional<&>prop_no_digit_no_math_punct\"] = sum(int(c in math_punct and not c.isdigit()) for c in a.replace(\" \", \"\"))/o[\"traditional<&>len(~A)\"]\n",
    "    o[\"traditional<&>num_alpha\"] = sum(int(t.isalpha()) for t in a.replace(\" \", \"\"))\n",
    "    o[\"traditional<&>prop_alpha_vowels\"] = sum(int(t in \"aeiou\") for t in a.replace(\" \", \"\"))/o[\"traditional<&>num_alpha\"] if o[\"traditional<&>num_alpha\"]>0 else 0\n",
    "    \n",
    "    return o\n",
    "\n",
    "def get_overlap_by(dic_a, dic_q, by=\"\"): #Q[propn+]&A, Q[quién|cuál|qué], Q[ser&posible], A[binary(si|no)]\n",
    "    theta = 0.7\n",
    "    injection_index = 0\n",
    "    if \"Q\" in by and \"A\" not in by:\n",
    "        q_lemma = [w.replace(\",\", \"\").replace(\".\", \"\") for w in dic_q[\"lf_lemma\"]]\n",
    "        if by == \"Q[quién|cuál|qué]\":\n",
    "            if \"quién\" in q_lemma or \"cuál\" in q_lemma or \"qué\" in q_lemma:\n",
    "                injection_index = 1\n",
    "        elif by == \"Q[ser&posible]\":\n",
    "            if \"ser\" in q_lemma and \"posible\" in q_lemma:\n",
    "                injection_index = 1\n",
    "        elif by == \"Q[(ser*&correcto)|(tener&razón)]\":\n",
    "            if ((\"ser\" in q_lemma or \"es\" in q_lemma or \"estar\" in q_lemma or \"este\" in q_lemma or \"esta\" in q_lemma) and \"correcto\" in q_lemma) or (\"tener\" in q_lemma and \"razón\" in q_lemma):\n",
    "                injection_index = 1\n",
    "        elif by == \"Q[ser*&correcto]\":\n",
    "            if (\"ser\" in q_lemma or \"es\" in q_lemma or \"estar\" in q_lemma or \"este\" in q_lemma or \"esta\" in q_lemma) and \"correcto\" in q_lemma:\n",
    "                injection_index = 1\n",
    "        elif  by == \"Q[tener&razón]\":\n",
    "             if \"tener\" in q_lemma and \"razón\" in q_lemma:\n",
    "                injection_index = 1\n",
    "        elif by == \"Q[quién|cuál]\":\n",
    "            if \"quién\" in q_lemma or \"cuál\":\n",
    "                injection_index = 1\n",
    "        elif by == \"Q[ser*&bien]\":\n",
    "            if (\"ser\" in q_lemma or \"es\" in q_lemma or \"estar\" in q_lemma or \"este\" in q_lemma or \"esta\" in q_lemma) and \"bien\" in q_lemma:\n",
    "                injection_index = 1\n",
    "        elif by == \"Q[por&qué&ser*&equivocado]\":\n",
    "            if \"por\" in q_lemma and \"qué\" in q_lemma and (\"ser\" in q_lemma or \"es\" in q_lemma or \"estar\" in q_lemma or \"este\" in q_lemma or \"esta\" in q_lemma) and (\"equivocado\" in q_lemma or \"equivocada\" in q_lemma):\n",
    "                injection_index = 1\n",
    "        elif by == \"Q[por&qué&equivocado]\":\n",
    "            if \"por\" in q_lemma and \"qué\" in q_lemma  and (\"equivocado\" in q_lemma or \"equivocada\" in q_lemma):\n",
    "                injection_index = 1\n",
    "        \n",
    "        \n",
    "    elif \"A\" in by and \"Q\" not in by:\n",
    "        tokens_a = set(dic_a[\"org_tokens\"]+dic_a[\"tokens\"]+dic_a[\"lower_org_tokens\"]+dic_a[\"org_tokens_wo_punct\"])\n",
    "        if by == \"A[binary(si|no)]\":\n",
    "            for k, t in enumerate(tokens_a):\n",
    "                t = t.replace(\",\", \"\").replace(\".\", \"\")\n",
    "                for s in [\"verdadero\", \"falso\", \"bien\", \"mal\", \"si\", \"no\", \"correcta\", \"correcto\", \"equivocada\", \"equivocado\", \"incorrecta\", \"confundida\", \"confundido\", \"incorrecto\", \"razon\", \"razón\", \"sip\", \"nop\", \"estamal\", \"estabien\"]:\n",
    "                    if sim_lev(s, t) >= theta:\n",
    "                        injection_index += 1\n",
    "                        break\n",
    "        if by == \"A[porque|binary(si, no)]\":\n",
    "            for k, t in enumerate(tokens_a):\n",
    "                t = t.replace(\",\", \"\").replace(\".\", \"\")\n",
    "                for s in [\"porque\", \"por\", \"que\", \"bien\", \"sip\", \"nop\", \"estamal\", \"estabien\", \"mal\", \"si\", \"no\", \"correcta\", \"correcto\", \"confundida\", \"confundido\", \"equivocada\", \"equivocado\", \"incorrecta\", \"incorrecto\", \"razon\", \"razón\"]:\n",
    "                    if sim_lev(s, t) >= theta:\n",
    "                        injection_index += 1\n",
    "                        break\n",
    "    elif \"A\" in by and \"Q\" in by:\n",
    "        if by == \"Q[rel_subj]+&A+\":\n",
    "            propn_q = dic_q[\"rel_subj\"]+[\"ninguno\",\"ninguna\", \"todos\",\"todas\", \"ambos\",\"ambas\", \"nadie\", \"alguno\", \"alguna\"]\n",
    "            tokens_a = set(dic_a[\"org_tokens\"]+dic_a[\"tokens\"]+dic_a[\"aux_tokens\"]+dic_a[\"lower_org_tokens\"]+dic_a[\"org_tokens_wo_punct\"])\n",
    "            for k, t in enumerate(tokens_a):\n",
    "                t = t.replace(\",\", \"\").replace(\".\", \"\")\n",
    "                for s in propn_q:\n",
    "                    if sim_lev(s, t) >= theta:\n",
    "                        injection_index += 1\n",
    "                        break\n",
    "        if by == \"Q[propn]+&A+\":\n",
    "            propn_q = dic_q[\"rel_subj\"]+[\"ninguno\",\"ninguna\", \"todos\",\"todas\", \"ambos\",\"ambas\", \"nadie\", \"alguno\", \"alguna\"]\n",
    "            tokens_a = set(dic_a[\"org_tokens\"]+dic_a[\"tokens\"]+dic_a[\"aux_tokens\"]+dic_a[\"lower_org_tokens\"]+dic_a[\"org_tokens_wo_punct\"])\n",
    "            for k, t in enumerate(tokens_a):\n",
    "                t = t.replace(\",\", \"\").replace(\".\", \"\")\n",
    "                for s in propn_q:\n",
    "                    if sim_lev(s, t) >= theta:\n",
    "                        injection_index += 1\n",
    "                        break\n",
    "    return injection_index\n",
    "\n",
    "class Preprocessing(object):\n",
    "    def __init__(self):\n",
    "        self.dic_comparison = {}\n",
    "        self.dic_lf = {}\n",
    "        self.dic_topo = {}\n",
    "        self.dic_attrib = {\"a\": {}, \"q\": {}}\n",
    "        self.dic_features = {}\n",
    "        self.dic_simple_topo = {}\n",
    "    \n",
    "    def get_dic_attrib(self, t, ix, a):\n",
    "        if ix not in self.dic_attrib[t].keys():    \n",
    "            self.dic_attrib[t][ix] = get_attrib(a)\n",
    "        return self.dic_attrib[t][ix]\n",
    "            \n",
    "    def get_lf(self, ix, a):\n",
    "        a = \"\" if a == \"nan\" else a\n",
    "        a = a.replace(\"\\n\", \" \")\n",
    "        a = \" \".join(a.split())\n",
    "        if ix not in self.dic_lf.keys():\n",
    "            o = {}\n",
    "            doc_a = nlp(a)   \n",
    "            num_tokens = len(doc_a)\n",
    "            o[\"traditional<&>num_tokens\"] = num_tokens\n",
    "            for token in doc_a:\n",
    "                dic_lf = {\n",
    "#                     \"lemma\": token.lemma_,\n",
    "                    \"tag\": token.tag_,\n",
    "                    \"dep\": token.dep_,\n",
    "                    \"shape\": token.shape_,\n",
    "                    \"is_alpha\": token.is_alpha,\n",
    "                    \"is_stop\": token.is_stop\n",
    "                }\n",
    "                for k, v in dic_lf.items():\n",
    "                    name_col = f\"linguistic<&>{k}<&>{v}\"\n",
    "                    if name_col not in o.keys():\n",
    "                        o[name_col] = 0\n",
    "                    o[name_col] += 1\n",
    "                    \n",
    "                    if k == \"shape\":\n",
    "                        if v != \"\" and unidecode(v) == \"\":\n",
    "                            name_col = f\"linguistic<&>{k}<&>emoji\"\n",
    "                            if name_col not in o.keys():\n",
    "                                o[name_col] = 0\n",
    "                            o[name_col] += 1 \n",
    "                        else:\n",
    "                            for c in set(v):\n",
    "                                name_col = f\"linguistic<&>{k}<&>contains({c})\"\n",
    "                                if c in \"\"\"!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\"\"\":\n",
    "                                    name_col = f\"linguistic<&>{k}<&>contains(punct)\"\n",
    "                                \n",
    "                                if name_col not in o.keys():\n",
    "                                    o[name_col] = 0\n",
    "                                o[name_col] += 1\n",
    "\n",
    "            for k, v in o.copy().items():\n",
    "                if (k != \"traditional<&>num_tokens\") and (\"linguistic<&>lemma<&>\" not in k) and ((\"linguistic<&>shape<&>\" not in k) or (\"linguistic<&>shape<&>\" in k and k.split(\"<&>\")[2] in [\"emoji\", \"contains(x)\", \"contains(d)\", \"contains(punct)\"])):\n",
    "                    nk = k.replace(\"traditional<&>\", \"\").replace(\"linguistic<&>\", \"\")\n",
    "                    o[f\"linguistic<&>ratio({nk}/num_tokens)\"] = v/o[\"traditional<&>num_tokens\"] if o[\"traditional<&>num_tokens\"]!= 0 else 0\n",
    "                    \n",
    "            self.dic_lf[ix] = o\n",
    "        return self.dic_lf[ix]\n",
    "        \n",
    "    def get_representation(self, ixa, ixq, a, q):\n",
    "        ix = f\"{ixa}_{ixq}\" \n",
    "        if ix not in self.dic_comparison.keys():\n",
    "            dic_a = self.get_dic_attrib(\"a\", ixa, a)  \n",
    "            dic_q = self.get_dic_attrib(\"q\", ixq, q)  \n",
    "            o = {}\n",
    "            o[\"contextual<&>Q[rel_subj]+&A+\"] = get_overlap_by(dic_a, dic_q, \"Q[rel_subj]+&A+\")\n",
    "            o[\"contextual<&>Q[quién|cuál|qué]\"] = get_overlap_by(None, dic_q, \"Q[quién|cuál|qué]\")\n",
    "            o[\"contextual<&>Q[ser&posible]\"] = get_overlap_by(None, dic_q, \"Q[ser&posible]\")\n",
    "            o[\"contextual<&>A[binary(si|no)]\"] = get_overlap_by(dic_a, None, \"A[binary(si|no)]\")\n",
    "\n",
    "            o[\"contextual<&>Q[por&qué&equivocado]\"] = get_overlap_by(None, dic_q, \"Q[por&qué&equivocado]\")\n",
    "            o[\"contextual<&>Q[ser*&bien]\"] = get_overlap_by(None, dic_q, \"Q[ser*&bien]\")\n",
    "            o[\"contextual<&>A[porque|binary(si|no)]\"] = get_overlap_by(dic_a, None, \"A[porque|binary(si|no)]\")\n",
    "            o[\"contextual<&>Q[propn]+&A+\"] = get_overlap_by(dic_a, dic_q, \"Q[propn]+&A+\")\n",
    "            o[\"contextual<&>Q[quién|cuál]\"] = get_overlap_by(None, dic_q, \"Q[quién|cuál]\")\n",
    "            o[\"contextual<&>Q[(ser*&correcto)|(tener&razón)]\"] = get_overlap_by(None, dic_q, \"Q[(ser*&correcto)|(tener&razón)]\")\n",
    "            o[\"contextual<&>Q[ser*&correcto]\"] = get_overlap_by(None, dic_q, \"Q[ser*&correcto]\")\n",
    "            o[\"contextual<&>Q[tener&razón]\"]= get_overlap_by(None, dic_q, \"Q[tener&razón]\")\n",
    "\n",
    "            o[\"contextual<&>injection_index\"] = get_injection_index(str(a), str(q))\n",
    "            self.dic_comparison[ix] = o\n",
    "        return self.dic_comparison[ix] \n",
    "            \n",
    "    def get_preprocessing(self, D):\n",
    "        data_representations = []\n",
    "        _times = []\n",
    "        for k, ixa in enumerate(D.index):\n",
    "            start = time.time()\n",
    "            a = D.loc[ixa][\"A\"]\n",
    "            a = str(a)\n",
    "            q = D.loc[ixa][\"Q\"]\n",
    "            q = str(q) \n",
    "            ixq = D.loc[ixa][\"Q_id\"]\n",
    "            o = self.get_features(ixa, ixq, a, q)\n",
    "            data_representations.append(o)\n",
    "            end = time.time()\n",
    "            _times.append(end-start)\n",
    "            time_expected = (len(D.index)-(k+1))*np.mean(_times)\n",
    "            time_expected_min = np.floor(time_expected/60)\n",
    "            time_expected_sec = time_expected - time_expected_min*60\n",
    "            if k % 100 == 0 or k==len(D.index)-1:\n",
    "                print(f\"\"\"{k+1}/{len(D.index)}, progress: {100*(k+1)/len(D.index): .2f} %, dt: {_times[-1]: .2f}, exp. dt: {np.mean(_times): .2f} p/m {np.std(_times): .2f} s, t. trans: {np.sum(_times)/60: .1f} min, t. exp. end: {time_expected_min: .1f} m {time_expected_sec: .1f} s\"\"\")            \n",
    "        df_representation = pd.DataFrame(data_representations, index=D.index)\n",
    "        fillna_0 = [\"linguistic<&>\"]\n",
    "        dic_fillna = {c: 0 for c in df_representation.columns if any(x in c for x in fillna_0)}\n",
    "        df_representation = df_representation.fillna(dic_fillna)\n",
    "        return df_representation\n",
    "    \n",
    "    def get_topo(self, ix, a):\n",
    "        if ix not in self.dic_topo.keys():\n",
    "            self.dic_topo[ix] = get_topo_features(a)\n",
    "        return self.dic_topo[ix]   \n",
    "     \n",
    "    def get_simple_topo(self, ix, a):\n",
    "        if ix not in self.dic_simple_topo.keys():\n",
    "            dic_a = self.get_dic_attrib(\"a\", ix, a)   \n",
    "            self.dic_simple_topo[ix] = get_simple_topo(dic_a)\n",
    "        return self.dic_simple_topo[ix]   \n",
    "        \n",
    "    def get_features(self, ixa, ixq, a, q):\n",
    "            ix = f\"{ixa}_{ixq}\"\n",
    "            if ix not in self.dic_features.keys():\n",
    "                o = self.get_representation(ixa, ixq, a, q)\n",
    "                \n",
    "                o = {**o, **self.get_topo(ixa, a)}\n",
    "                o = {**o, **self.get_lf(ixa, a)}\n",
    "                dic_a = self.get_dic_attrib(\"a\", ixa, a)   \n",
    "                for k, v in self.get_simple_topo(ixa, a).items():\n",
    "                    if k in [\n",
    "                             'len',\n",
    "                             'num_digit',\n",
    "                             'num_math_punct',\n",
    "                             'num_no_numbers',\n",
    "                             'num_numbers',\n",
    "                             'num_punct',\n",
    "                             'num_tokens',\n",
    "                    ]: o[\"traditional<&>\"+k] = v\n",
    "                    elif k in [\n",
    "                        'num_keywords',\n",
    "                        'num_rae',\n",
    "                        'num_ud',\n",
    "                        'num_slang',\n",
    "                        'num_faces'\n",
    "                    ]: o[\"semantic<&>\"+k] = v\n",
    "                    \n",
    "                o[\"semantic<&>ratio_rae\"] = get_ratio_rae_ud(dic_a, \"rae\")\n",
    "                o[\"semantic<&>ratio_ud\"] = get_ratio_rae_ud(dic_a, \"ud\")                \n",
    "                o[\"semantic<&>ratio_slang\"] = get_ratio_slang(dic_a)\n",
    "                o[\"semantic<&>ratio_keywords\"] = get_ratio_keywords(dic_a)\n",
    "                o[\"semantic<&>ratio_faces\"] = get_ratio_faces(dic_a)\n",
    "                \n",
    "                o[\"traditional<&>ratio_vowel\"] = get_ratio_vowel(dic_a)\n",
    "                o[\"traditional<&>ratio_no_numbers\"] = get_ratio_no_numbers(dic_a)\n",
    "                o[\"traditional<&>ratio_punct\"] =  get_ratio_punct(dic_a, default=True)\n",
    "                o[\"traditional<&>exist_numbs\"] =  get_exist_numbs(dic_a)\n",
    "                o[\"traditional<&>max_len_number\"] = get_max_len_number(dic_a)\n",
    "                \n",
    "                self.dic_features[ix] = o\n",
    "            return self.dic_features[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ad6b0cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/11559, progress:  0.01 %, dt:  0.12, exp. dt:  0.12 p/m  0.00 s, t. trans:  0.0 min, t. exp. end:  22.0 m  33.5 s\n",
      "101/11559, progress:  0.87 %, dt:  0.05, exp. dt:  0.08 p/m  0.03 s, t. trans:  0.1 min, t. exp. end:  15.0 m  21.4 s\n",
      "201/11559, progress:  1.74 %, dt:  0.05, exp. dt:  0.07 p/m  0.03 s, t. trans:  0.2 min, t. exp. end:  13.0 m  20.4 s\n",
      "301/11559, progress:  2.60 %, dt:  0.05, exp. dt:  0.07 p/m  0.02 s, t. trans:  0.3 min, t. exp. end:  12.0 m  15.8 s\n",
      "401/11559, progress:  3.47 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.4 min, t. exp. end:  11.0 m  39.5 s\n",
      "501/11559, progress:  4.33 %, dt:  0.04, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.5 min, t. exp. end:  11.0 m  18.9 s\n",
      "601/11559, progress:  5.20 %, dt:  0.06, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.6 min, t. exp. end:  10.0 m  59.8 s\n",
      "701/11559, progress:  6.06 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.7 min, t. exp. end:  10.0 m  41.5 s\n",
      "801/11559, progress:  6.93 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.8 min, t. exp. end:  10.0 m  25.5 s\n",
      "901/11559, progress:  7.79 %, dt:  0.06, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.9 min, t. exp. end:  10.0 m  11.4 s\n",
      "1001/11559, progress:  8.66 %, dt:  0.06, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.9 min, t. exp. end:  9.0 m  59.7 s\n",
      "1101/11559, progress:  9.53 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.0 min, t. exp. end:  9.0 m  48.4 s\n",
      "1201/11559, progress:  10.39 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.1 min, t. exp. end:  9.0 m  38.0 s\n",
      "1301/11559, progress:  11.26 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.2 min, t. exp. end:  9.0 m  27.3 s\n",
      "1401/11559, progress:  12.12 %, dt:  0.04, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.3 min, t. exp. end:  9.0 m  18.9 s\n",
      "1501/11559, progress:  12.99 %, dt:  0.06, exp. dt:  0.05 p/m  0.01 s, t. trans:  1.4 min, t. exp. end:  9.0 m  10.8 s\n",
      "1601/11559, progress:  13.85 %, dt:  0.05, exp. dt:  0.05 p/m  0.01 s, t. trans:  1.5 min, t. exp. end:  9.0 m  2.3 s\n",
      "1701/11559, progress:  14.72 %, dt:  0.05, exp. dt:  0.05 p/m  0.01 s, t. trans:  1.5 min, t. exp. end:  8.0 m  54.2 s\n",
      "1801/11559, progress:  15.58 %, dt:  0.05, exp. dt:  0.05 p/m  0.01 s, t. trans:  1.6 min, t. exp. end:  8.0 m  47.2 s\n",
      "1901/11559, progress:  16.45 %, dt:  0.04, exp. dt:  0.05 p/m  0.01 s, t. trans:  1.7 min, t. exp. end:  8.0 m  39.4 s\n",
      "2001/11559, progress:  17.31 %, dt:  0.06, exp. dt:  0.05 p/m  0.01 s, t. trans:  1.8 min, t. exp. end:  8.0 m  33.3 s\n",
      "2101/11559, progress:  18.18 %, dt:  0.06, exp. dt:  0.05 p/m  0.01 s, t. trans:  1.9 min, t. exp. end:  8.0 m  27.1 s\n",
      "2201/11559, progress:  19.04 %, dt:  0.06, exp. dt:  0.05 p/m  0.01 s, t. trans:  2.0 min, t. exp. end:  8.0 m  22.4 s\n",
      "2301/11559, progress:  19.91 %, dt:  0.05, exp. dt:  0.05 p/m  0.01 s, t. trans:  2.1 min, t. exp. end:  8.0 m  17.1 s\n",
      "2401/11559, progress:  20.77 %, dt:  0.06, exp. dt:  0.05 p/m  0.01 s, t. trans:  2.1 min, t. exp. end:  8.0 m  12.0 s\n",
      "2501/11559, progress:  21.64 %, dt:  0.06, exp. dt:  0.05 p/m  0.01 s, t. trans:  2.2 min, t. exp. end:  8.0 m  7.0 s\n",
      "2601/11559, progress:  22.50 %, dt:  0.05, exp. dt:  0.05 p/m  0.01 s, t. trans:  2.3 min, t. exp. end:  8.0 m  2.3 s\n",
      "2701/11559, progress:  23.37 %, dt:  0.05, exp. dt:  0.05 p/m  0.01 s, t. trans:  2.4 min, t. exp. end:  7.0 m  56.8 s\n",
      "2801/11559, progress:  24.23 %, dt:  0.05, exp. dt:  0.05 p/m  0.01 s, t. trans:  2.5 min, t. exp. end:  7.0 m  51.0 s\n",
      "2901/11559, progress:  25.10 %, dt:  0.07, exp. dt:  0.05 p/m  0.01 s, t. trans:  2.6 min, t. exp. end:  7.0 m  45.6 s\n",
      "3001/11559, progress:  25.96 %, dt:  0.06, exp. dt:  0.05 p/m  0.01 s, t. trans:  2.7 min, t. exp. end:  7.0 m  40.6 s\n",
      "3101/11559, progress:  26.83 %, dt:  0.05, exp. dt:  0.05 p/m  0.01 s, t. trans:  2.8 min, t. exp. end:  7.0 m  35.2 s\n",
      "3201/11559, progress:  27.69 %, dt:  0.06, exp. dt:  0.05 p/m  0.01 s, t. trans:  2.9 min, t. exp. end:  7.0 m  29.6 s\n",
      "3301/11559, progress:  28.56 %, dt:  0.04, exp. dt:  0.05 p/m  0.01 s, t. trans:  3.0 min, t. exp. end:  7.0 m  22.9 s\n",
      "3401/11559, progress:  29.42 %, dt:  0.04, exp. dt:  0.05 p/m  0.01 s, t. trans:  3.0 min, t. exp. end:  7.0 m  15.2 s\n",
      "3501/11559, progress:  30.29 %, dt:  0.04, exp. dt:  0.05 p/m  0.01 s, t. trans:  3.1 min, t. exp. end:  7.0 m  7.7 s\n",
      "3601/11559, progress:  31.15 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  3.2 min, t. exp. end:  7.0 m  2.8 s\n",
      "3701/11559, progress:  32.02 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  3.3 min, t. exp. end:  6.0 m  55.7 s\n",
      "3801/11559, progress:  32.88 %, dt:  0.07, exp. dt:  0.05 p/m  0.02 s, t. trans:  3.3 min, t. exp. end:  6.0 m  48.6 s\n",
      "3901/11559, progress:  33.75 %, dt:  0.09, exp. dt:  0.05 p/m  0.02 s, t. trans:  3.5 min, t. exp. end:  6.0 m  47.6 s\n",
      "4001/11559, progress:  34.61 %, dt:  0.06, exp. dt:  0.05 p/m  0.02 s, t. trans:  3.6 min, t. exp. end:  6.0 m  44.7 s\n",
      "4101/11559, progress:  35.48 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  3.7 min, t. exp. end:  6.0 m  39.6 s\n",
      "4201/11559, progress:  36.34 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  3.8 min, t. exp. end:  6.0 m  34.3 s\n",
      "4301/11559, progress:  37.21 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  3.8 min, t. exp. end:  6.0 m  28.7 s\n",
      "4401/11559, progress:  38.07 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  3.9 min, t. exp. end:  6.0 m  23.1 s\n",
      "4501/11559, progress:  38.94 %, dt:  0.07, exp. dt:  0.05 p/m  0.02 s, t. trans:  4.0 min, t. exp. end:  6.0 m  17.5 s\n",
      "4601/11559, progress:  39.80 %, dt:  0.06, exp. dt:  0.05 p/m  0.02 s, t. trans:  4.1 min, t. exp. end:  6.0 m  12.2 s\n",
      "4701/11559, progress:  40.67 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  4.2 min, t. exp. end:  6.0 m  7.0 s\n",
      "4801/11559, progress:  41.53 %, dt:  0.06, exp. dt:  0.05 p/m  0.02 s, t. trans:  4.3 min, t. exp. end:  6.0 m  1.5 s\n",
      "4901/11559, progress:  42.40 %, dt:  0.06, exp. dt:  0.05 p/m  0.02 s, t. trans:  4.4 min, t. exp. end:  5.0 m  56.2 s\n",
      "5001/11559, progress:  43.26 %, dt:  0.06, exp. dt:  0.05 p/m  0.02 s, t. trans:  4.5 min, t. exp. end:  5.0 m  50.6 s\n",
      "5101/11559, progress:  44.13 %, dt:  0.07, exp. dt:  0.05 p/m  0.02 s, t. trans:  4.5 min, t. exp. end:  5.0 m  45.2 s\n",
      "5201/11559, progress:  45.00 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  4.6 min, t. exp. end:  5.0 m  40.0 s\n",
      "5301/11559, progress:  45.86 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  4.7 min, t. exp. end:  5.0 m  34.5 s\n",
      "5401/11559, progress:  46.73 %, dt:  0.06, exp. dt:  0.05 p/m  0.02 s, t. trans:  4.8 min, t. exp. end:  5.0 m  29.0 s\n",
      "5501/11559, progress:  47.59 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  4.9 min, t. exp. end:  5.0 m  23.5 s\n",
      "5601/11559, progress:  48.46 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  5.0 min, t. exp. end:  5.0 m  17.9 s\n",
      "5701/11559, progress:  49.32 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  5.1 min, t. exp. end:  5.0 m  12.2 s\n",
      "5801/11559, progress:  50.19 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  5.1 min, t. exp. end:  5.0 m  6.7 s\n",
      "5901/11559, progress:  51.05 %, dt:  0.06, exp. dt:  0.05 p/m  0.02 s, t. trans:  5.2 min, t. exp. end:  5.0 m  1.2 s\n",
      "6001/11559, progress:  51.92 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  5.3 min, t. exp. end:  4.0 m  55.6 s\n",
      "6101/11559, progress:  52.78 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  5.4 min, t. exp. end:  4.0 m  50.0 s\n",
      "6201/11559, progress:  53.65 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  5.5 min, t. exp. end:  4.0 m  44.5 s\n",
      "6301/11559, progress:  54.51 %, dt:  0.06, exp. dt:  0.05 p/m  0.02 s, t. trans:  5.6 min, t. exp. end:  4.0 m  38.9 s\n",
      "6401/11559, progress:  55.38 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  5.7 min, t. exp. end:  4.0 m  33.6 s\n",
      "6501/11559, progress:  56.24 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  5.7 min, t. exp. end:  4.0 m  28.2 s\n",
      "6601/11559, progress:  57.11 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  5.8 min, t. exp. end:  4.0 m  22.6 s\n",
      "6701/11559, progress:  57.97 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  5.9 min, t. exp. end:  4.0 m  17.0 s\n",
      "6801/11559, progress:  58.84 %, dt:  0.06, exp. dt:  0.05 p/m  0.02 s, t. trans:  6.0 min, t. exp. end:  4.0 m  11.2 s\n",
      "6901/11559, progress:  59.70 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  6.1 min, t. exp. end:  4.0 m  5.6 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7001/11559, progress:  60.57 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  6.1 min, t. exp. end:  4.0 m  0.0 s\n",
      "7101/11559, progress:  61.43 %, dt:  0.10, exp. dt:  0.05 p/m  0.02 s, t. trans:  6.2 min, t. exp. end:  3.0 m  54.5 s\n",
      "7201/11559, progress:  62.30 %, dt:  0.09, exp. dt:  0.05 p/m  0.02 s, t. trans:  6.3 min, t. exp. end:  3.0 m  50.0 s\n",
      "7301/11559, progress:  63.16 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  6.4 min, t. exp. end:  3.0 m  44.8 s\n",
      "7401/11559, progress:  64.03 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  6.5 min, t. exp. end:  3.0 m  39.3 s\n",
      "7501/11559, progress:  64.89 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  6.6 min, t. exp. end:  3.0 m  33.7 s\n",
      "7601/11559, progress:  65.76 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  6.7 min, t. exp. end:  3.0 m  28.2 s\n",
      "7701/11559, progress:  66.62 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  6.7 min, t. exp. end:  3.0 m  22.6 s\n",
      "7801/11559, progress:  67.49 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  6.8 min, t. exp. end:  3.0 m  17.1 s\n",
      "7901/11559, progress:  68.35 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  6.9 min, t. exp. end:  3.0 m  11.5 s\n",
      "8001/11559, progress:  69.22 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  7.0 min, t. exp. end:  3.0 m  6.0 s\n",
      "8101/11559, progress:  70.08 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  7.0 min, t. exp. end:  3.0 m  0.6 s\n",
      "8201/11559, progress:  70.95 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  7.1 min, t. exp. end:  2.0 m  55.2 s\n",
      "8301/11559, progress:  71.81 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  7.2 min, t. exp. end:  2.0 m  49.7 s\n",
      "8401/11559, progress:  72.68 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  7.3 min, t. exp. end:  2.0 m  44.4 s\n",
      "8501/11559, progress:  73.54 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  7.4 min, t. exp. end:  2.0 m  39.0 s\n",
      "8601/11559, progress:  74.41 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  7.4 min, t. exp. end:  2.0 m  33.5 s\n",
      "8701/11559, progress:  75.27 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  7.5 min, t. exp. end:  2.0 m  28.6 s\n",
      "8801/11559, progress:  76.14 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  7.6 min, t. exp. end:  2.0 m  23.6 s\n",
      "8901/11559, progress:  77.00 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  7.7 min, t. exp. end:  2.0 m  18.4 s\n",
      "9001/11559, progress:  77.87 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  7.8 min, t. exp. end:  2.0 m  13.3 s\n",
      "9101/11559, progress:  78.74 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  7.9 min, t. exp. end:  2.0 m  8.1 s\n",
      "9201/11559, progress:  79.60 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  8.0 min, t. exp. end:  2.0 m  2.9 s\n",
      "9301/11559, progress:  80.47 %, dt:  0.06, exp. dt:  0.05 p/m  0.02 s, t. trans:  8.1 min, t. exp. end:  1.0 m  57.7 s\n",
      "9401/11559, progress:  81.33 %, dt:  0.07, exp. dt:  0.05 p/m  0.02 s, t. trans:  8.2 min, t. exp. end:  1.0 m  52.5 s\n",
      "9501/11559, progress:  82.20 %, dt:  0.07, exp. dt:  0.05 p/m  0.02 s, t. trans:  8.3 min, t. exp. end:  1.0 m  47.3 s\n",
      "9601/11559, progress:  83.06 %, dt:  0.09, exp. dt:  0.05 p/m  0.02 s, t. trans:  8.3 min, t. exp. end:  1.0 m  42.2 s\n",
      "9701/11559, progress:  83.93 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  8.4 min, t. exp. end:  1.0 m  37.0 s\n",
      "9801/11559, progress:  84.79 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  8.5 min, t. exp. end:  1.0 m  31.7 s\n",
      "9901/11559, progress:  85.66 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  8.6 min, t. exp. end:  1.0 m  26.4 s\n",
      "10001/11559, progress:  86.52 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  8.7 min, t. exp. end:  1.0 m  21.4 s\n",
      "10101/11559, progress:  87.39 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  8.8 min, t. exp. end:  1.0 m  16.2 s\n",
      "10201/11559, progress:  88.25 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  8.9 min, t. exp. end:  1.0 m  11.0 s\n",
      "10301/11559, progress:  89.12 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  9.0 min, t. exp. end:  1.0 m  5.7 s\n",
      "10401/11559, progress:  89.98 %, dt:  0.06, exp. dt:  0.05 p/m  0.02 s, t. trans:  9.1 min, t. exp. end:  1.0 m  0.5 s\n",
      "10501/11559, progress:  90.85 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  9.1 min, t. exp. end:  0.0 m  55.3 s\n",
      "10601/11559, progress:  91.71 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  9.2 min, t. exp. end:  0.0 m  50.1 s\n",
      "10701/11559, progress:  92.58 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  9.3 min, t. exp. end:  0.0 m  44.9 s\n",
      "10801/11559, progress:  93.44 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  9.4 min, t. exp. end:  0.0 m  39.6 s\n",
      "10901/11559, progress:  94.31 %, dt:  0.06, exp. dt:  0.05 p/m  0.02 s, t. trans:  9.5 min, t. exp. end:  0.0 m  34.4 s\n",
      "11001/11559, progress:  95.17 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  9.6 min, t. exp. end:  0.0 m  29.2 s\n",
      "11101/11559, progress:  96.04 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  9.7 min, t. exp. end:  0.0 m  23.9 s\n",
      "11201/11559, progress:  96.90 %, dt:  0.10, exp. dt:  0.05 p/m  0.02 s, t. trans:  9.8 min, t. exp. end:  0.0 m  18.7 s\n",
      "11301/11559, progress:  97.77 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  9.9 min, t. exp. end:  0.0 m  13.5 s\n",
      "11401/11559, progress:  98.63 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  9.9 min, t. exp. end:  0.0 m  8.3 s\n",
      "11501/11559, progress:  99.50 %, dt:  0.06, exp. dt:  0.05 p/m  0.02 s, t. trans:  10.0 min, t. exp. end:  0.0 m  3.0 s\n",
      "11559/11559, progress:  100.00 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  10.1 min, t. exp. end:  0.0 m  0.0 s\n",
      "Wall time: 10min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_preproc = Preprocessing()\n",
    "df_features_train = train_preproc.get_preprocessing(A_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a8999db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2898, progress:  0.03 %, dt:  0.10, exp. dt:  0.10 p/m  0.00 s, t. trans:  0.0 min, t. exp. end:  4.0 m  52.9 s\n",
      "101/2898, progress:  3.49 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.1 min, t. exp. end:  2.0 m  39.8 s\n",
      "201/2898, progress:  6.94 %, dt:  0.08, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.2 min, t. exp. end:  2.0 m  28.5 s\n",
      "301/2898, progress:  10.39 %, dt:  0.04, exp. dt:  0.05 p/m  0.02 s, t. trans:  0.3 min, t. exp. end:  2.0 m  22.4 s\n",
      "401/2898, progress:  13.84 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.4 min, t. exp. end:  2.0 m  19.3 s\n",
      "501/2898, progress:  17.29 %, dt:  0.06, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.5 min, t. exp. end:  2.0 m  13.2 s\n",
      "601/2898, progress:  20.74 %, dt:  0.06, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.6 min, t. exp. end:  2.0 m  7.1 s\n",
      "701/2898, progress:  24.19 %, dt:  0.04, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.6 min, t. exp. end:  2.0 m  1.0 s\n",
      "801/2898, progress:  27.64 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  0.7 min, t. exp. end:  1.0 m  54.6 s\n",
      "901/2898, progress:  31.09 %, dt:  0.05, exp. dt:  0.05 p/m  0.02 s, t. trans:  0.8 min, t. exp. end:  1.0 m  49.0 s\n",
      "1001/2898, progress:  34.54 %, dt:  0.06, exp. dt:  0.06 p/m  0.02 s, t. trans:  0.9 min, t. exp. end:  1.0 m  44.4 s\n",
      "1101/2898, progress:  37.99 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.0 min, t. exp. end:  1.0 m  39.3 s\n",
      "1201/2898, progress:  41.44 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.1 min, t. exp. end:  1.0 m  33.6 s\n",
      "1301/2898, progress:  44.89 %, dt:  0.10, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.2 min, t. exp. end:  1.0 m  28.0 s\n",
      "1401/2898, progress:  48.34 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.3 min, t. exp. end:  1.0 m  22.7 s\n",
      "1501/2898, progress:  51.79 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.4 min, t. exp. end:  1.0 m  17.7 s\n",
      "1601/2898, progress:  55.24 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.5 min, t. exp. end:  1.0 m  12.1 s\n",
      "1701/2898, progress:  58.70 %, dt:  0.04, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.6 min, t. exp. end:  1.0 m  6.3 s\n",
      "1801/2898, progress:  62.15 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.7 min, t. exp. end:  1.0 m  0.8 s\n",
      "1901/2898, progress:  65.60 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.8 min, t. exp. end:  0.0 m  55.5 s\n",
      "2001/2898, progress:  69.05 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.8 min, t. exp. end:  0.0 m  49.7 s\n",
      "2101/2898, progress:  72.50 %, dt:  0.06, exp. dt:  0.06 p/m  0.02 s, t. trans:  1.9 min, t. exp. end:  0.0 m  44.1 s\n",
      "2201/2898, progress:  75.95 %, dt:  0.11, exp. dt:  0.06 p/m  0.02 s, t. trans:  2.0 min, t. exp. end:  0.0 m  38.6 s\n",
      "2301/2898, progress:  79.40 %, dt:  0.07, exp. dt:  0.06 p/m  0.02 s, t. trans:  2.1 min, t. exp. end:  0.0 m  33.2 s\n",
      "2401/2898, progress:  82.85 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  2.2 min, t. exp. end:  0.0 m  27.7 s\n",
      "2501/2898, progress:  86.30 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  2.3 min, t. exp. end:  0.0 m  22.2 s\n",
      "2601/2898, progress:  89.75 %, dt:  0.06, exp. dt:  0.06 p/m  0.02 s, t. trans:  2.4 min, t. exp. end:  0.0 m  16.6 s\n",
      "2701/2898, progress:  93.20 %, dt:  0.04, exp. dt:  0.06 p/m  0.02 s, t. trans:  2.5 min, t. exp. end:  0.0 m  11.0 s\n",
      "2801/2898, progress:  96.65 %, dt:  0.05, exp. dt:  0.06 p/m  0.02 s, t. trans:  2.6 min, t. exp. end:  0.0 m  5.4 s\n",
      "2898/2898, progress:  100.00 %, dt:  0.08, exp. dt:  0.06 p/m  0.02 s, t. trans:  2.7 min, t. exp. end:  0.0 m  0.0 s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_preproc = Preprocessing()\n",
    "df_features_val = val_preproc.get_preprocessing(A_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "28fe55b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/677, progress:  0.15 %, dt:  0.10, exp. dt:  0.10 p/m  0.00 s, t. trans:  0.0 min, t. exp. end:  1.0 m  6.3 s\n",
      "101/677, progress:  14.92 %, dt:  0.12, exp. dt:  0.09 p/m  0.02 s, t. trans:  0.2 min, t. exp. end:  0.0 m  52.1 s\n",
      "201/677, progress:  29.69 %, dt:  0.05, exp. dt:  0.09 p/m  0.02 s, t. trans:  0.3 min, t. exp. end:  0.0 m  41.7 s\n",
      "301/677, progress:  44.46 %, dt:  0.05, exp. dt:  0.09 p/m  0.02 s, t. trans:  0.4 min, t. exp. end:  0.0 m  32.1 s\n",
      "401/677, progress:  59.23 %, dt:  0.06, exp. dt:  0.08 p/m  0.02 s, t. trans:  0.5 min, t. exp. end:  0.0 m  22.5 s\n",
      "501/677, progress:  74.00 %, dt:  0.06, exp. dt:  0.08 p/m  0.03 s, t. trans:  0.7 min, t. exp. end:  0.0 m  14.0 s\n",
      "601/677, progress:  88.77 %, dt:  0.10, exp. dt:  0.08 p/m  0.03 s, t. trans:  0.8 min, t. exp. end:  0.0 m  5.9 s\n",
      "677/677, progress:  100.00 %, dt:  0.11, exp. dt:  0.08 p/m  0.03 s, t. trans:  0.9 min, t. exp. end:  0.0 m  0.0 s\n",
      "Wall time: 51.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_preproc = Preprocessing()\n",
    "df_features_test = val_preproc.get_preprocessing(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "39bdbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_train = set(df_features_train.columns)\n",
    "cols_val = set(df_features_val.columns)\n",
    "cols_test = set(df_features_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f2af3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_val = cols_val.difference(cols_train)\n",
    "drop_test = cols_test.difference(cols_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "64d2e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_val = cols_train.difference(cols_val)\n",
    "add_test = cols_train.difference(cols_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3f1bc91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_features_train = df_features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5c2bd50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_features_val = df_features_val.drop(columns=drop_val)\n",
    "zero_cols = pd.concat([mf_features_val.iloc[:, 0] * 0] * len(add_val), axis=1)\n",
    "zero_cols.columns = add_val\n",
    "mf_features_val = pd.concat([mf_features_val, zero_cols], axis=1)[df_features_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9b21a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_features_test = df_features_test.drop(columns=drop_test)\n",
    "zero_cols = pd.concat([mf_features_test.iloc[:, 0] * 0] * len(add_test), axis=1)\n",
    "zero_cols.columns = add_test\n",
    "mf_features_test = pd.concat([mf_features_test, zero_cols], axis=1)[df_features_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6de959f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(\n",
    "    (mf_features_train.columns == mf_features_val.columns) \n",
    "    & \n",
    "    (mf_features_val.columns== mf_features_test.columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0557feeb",
   "metadata": {},
   "source": [
    "## Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "08163a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mf_features_train.to_excel(\"../features/mf_features_train_task_C1.xlsx\")\n",
    "mf_features_val.to_excel(\"../features/mf_features_val_task_C1.xlsx\")\n",
    "mf_features_test.to_excel(\"../features/mf_features_test_task_C1.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
